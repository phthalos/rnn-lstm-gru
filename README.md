## 1. 개요
### 1-1. 작업환경
Windows 11 24H2, PyCharm 2025.2.4, conda 24.11.3, Python 3.10.13, torch 2.5.1, keras 2.10.0, matplotlib 3.10.7, numpy 1.26.4, tqdm 4.67.1, AMD Ryzen 5 7600, NVIDIA GeForce RTX 4060 Ti (드라이버 버전: 32.0.15.8157)

### 1-2. 데이터셋 설명
실험에 사용할 데이터는  다음과 같은 형태로 이루어져 있다.\
훈련용 리뷰의 개수 : 25,000개\
테스트용 리뷰의 개수 : 25,000개\
카테고리 : 2 (긍정적 리뷰, 부정적 리뷰)

첫 번째 훈련용 리뷰를 출력해 보면 다음과 같은 결과를 얻을 수 있다.\
첫 번째 훈련용 리뷰 : `[1, 14, 22, 16, 43, 530, … (생략)`\
첫 번째 훈련용 리뷰의 레이블 : `1`

훈련용 리뷰의 배열 내 숫자는 단어의 등장 빈도 순위를 표시하고 있다. 숫자가 작을수록 등장 빈도가 높다는 뜻이다.\
리뷰의 레이블은 긍정적인 리뷰, 또는 부정적인 리뷰를 `0` 또는 `1`로 표시한다. `1`은 긍정적이라는 뜻이다.\
IMDB 데이터셋에는 긍정적인 리뷰와 부정적인 리뷰가 균등하게 포함되어 있다.\
25,000개의 리뷰는 문장이므로 각각 길이가 다르다.

각각의 숫자가 실제로 어떤 단어인지 확인해 보았다.\
`imdb.get_word_index()`에 각 단어와 매핑된 정수가 들어있고, IMDB에서 `0`, `1`, `2`, `3`은 특별 토큰으로 취급하므로 4번 인덱스가 빈도수 상위 1등 단어이다.
```python
vocab = imdb.get_word_index()
index_to_word = {}


for key, value in vocab.items():
    index_to_word[value+3] = key


print('빈도수 상위 1등 단어 : {}'.format(index_to_word[4]))
# 빈도수 상위 1등 단어 : the


for index, token in enumerate(("<pad>", "<sos>", "<unk>")):
  index_to_word[index] = token


print(' '.join([index_to_word[index] for index in X_train[0]]))
# <sos> this film was just brilliant casting location ... (생략)
```
## 2. 각 모델의 특징
코드 작성에 앞서, 앞으로 구현할 각 모델의 구조와 차이점에 대해 알아보았다.
### 2-1. 순차 데이터, 시계열 데이터
 순서가 있는 데이터를 순차 데이터라고 부른다. 예를 들어, 꽃잎의 길이, 너비는 각각이 독립된 데이터이므로 꽃잎의 너비, 길이와 같이 순서를 바꿔도 괜찮다. 하지만 주식 시세, 문장과 같이 연속적인 데이터는 내용을 뒤섞으면 그 의미가 퇴색되어 버리므로 순서가 매우 중요하다. 이렇게 순서가 있는 데이터를 순차 데이터(sequential data)라고 한다. 

주식 시세, 심전도 신호 등과 같이 순차 데이터 중에서도 일정 시간 간격으로 배치된 데이터를 시계열 데이터(Time Series)라고 한다.
### 2-1. RNN
RNN은 입력층, 은닉층, 출력층으로 구성된다. (은닉층은 여러 층이 될 수 있다.) 하지만 단순히 입력층 → 은닉층 → 출력층으로 흐르는 것이 아닌, 시간의 흐름에 따라 순환한다. 현재의 은닉층값 `h(t)`은 직전 순간의 은닉층값 `h(t-1)`과 이번 순간의 입력값 `x(t)`에 따라 결정된다. 매 순간의 상태는 직전 순간의 상태에 영향을 받으므로 이전 정보가 기억된다고 할 수 있다. 또한, 직전 순간의 은닉층값이 현재의 은닉층값으로 다시 들어오는 정보 흐름을 축약하면 순환 형태(loop)로 표현할 수 있다. RNN의 이름은 이러한 순환에서 유래되었다.

RNN을 그림으로 표현한 모습에서 `U`, `W`, `V`는 가중치로 수식의 에 해당된다. 따라서 입력층 → 은닉층으로 가는 에지의 가중치 `U`, 이전 순간의 은닉층 → 이번 순간의 은닉층으로 가는 에지의 가중치 `W`, 은닉층 → 출력층으로 가는 가중치 `V`는 RNN의 매개변수이다. (표기 방법은 자료마다 차이가 있다.) 이들은 어느 순간이든 같은 것이 사용된다. 한 마디로, 가중치를 공유한다.

조금 전에 언급한 것과 같이 RNN은 이전 정보를 기억하지만, 어떤 정보가 중요한지 구분하지 않는다. 또한 시간이 지날수록 이전의 정보는 새로운 은닉층에 의해 쌓이고, 덮어씌워지므로 시간이 지남에 따라 기억이 희미해진다, 과거의 입력값의 영향력이 점점 약해진다고 할 수 있다. 이러한 한계를 극복하기 위해, 정보의 특징에 중요도를 부여하고 선별하여 기억하는 기능을 추가한 것이 LSTM이다.


## 3. 모델 구현
### 3-1. RNN
RNN은 MLP와 유사한 구조를 가지고 있다. 그러므로 RNN을 학습시키는 과정에서도 MLP가 쓰는 방식을 따라하려는데, RNN은 MLP와 달리 시간 정보를 가졌다는 차이점이 있으므로 MLP의 방식을 그대로 쓸 수는 없다. 따라서 약간 개조된 BPTT(back-propagation through time)라는 학습 알고리즘을 사용한다. RNN은 순환 구조를 가지므로 전체 손실을 구하려면 매 순간을 거슬러 역전파해야 하기 때문이다.\
전체 손실(loss)은 첫 순간부터 마지막 순간까지, 각각의 순간에서 발생하는 모든 손실들의 합계이다.

모델의 구현 및 작동 과정을 요약하자면, 다음과 같다.

1. 리뷰를 단어별로 토큰화하여 인덱스를 부여한다.
2. 단어를 벡터 형태로 변환한다. (전처리 완료)
3. 전처리가 완료된 데이터를 모델에 삽입한다.
4. 마지막 은닉층의 상태를 포착한다. (학습 완료)
5. 완전 연결 계층에 통과시켜, 다음 상태를 예측한다.
6. `matplotlib`를 이용하여 결과를 그래프로 출력한다.

## 4. 참고문헌
- (한빛) 기계 학습 - 오일석 지음
- (길벗) 딥러닝 파이토치 교과서 - 서지영 지음
- (길벗) 머신러닝 교과서 파이토치 편 - 세바스찬 라시카, 유시 (헤이든) 류, 바히드 미자리리 지음
- (이지스퍼블리싱) 정직하게 코딩하며 배우는 딥러닝 입문 - 박해선 지음
- (이지스퍼블리싱) 딥러닝 교과서 - 윤성진 지음
 -(위키독스) 딥러닝 파이토치 교과서
  - [03-02 토치텍스트 튜토리얼](https://wikidocs.net/60314)
  - [07-01 순환 신경망](https://wikidocs.net/60690)
  - [10-04 IMDB 리뷰 감성 분류하기](https://wikidocs.net/24586)
- (Medium) [Building an RNN for Sentiment Analysis with PyTorch](https://medium.com/@jainilgosalia/building-an-rnn-for-sentiment-analysis-with-pytorch-5b80d9142bb8)

